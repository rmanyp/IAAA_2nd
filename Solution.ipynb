{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aebf2c4-6cea-4761-8961-0e773a34c381",
   "metadata": {},
   "source": [
    "## iaaa-data \n",
    "output dir : iaaa-nii.\n",
    "read dicom series, save as nii files and add dicom tags to file train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff50fc2f-45a8-4e30-a662-d159f6513dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "ROOT_DATA_DIR = Path('/kaggle/input/iaaa-mri-challenge').expanduser().absolute()\n",
    "DATA_DIR = ROOT_DATA_DIR / 'data'\n",
    "LABELS_PATH = ROOT_DATA_DIR / 'train.csv'\n",
    "PREPARED_DATA_DIR = Path('/kaggle/working/').expanduser().absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82b9d60-3f4f-424e-9b4d-ffb6762cf5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import SimpleITK as sitk\n",
    "from PIL import Image\n",
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "\n",
    "def read_dicom_series(study_path: Path, series_instance_uid: Optional[str] = None) -> np.ndarray:\n",
    "    \"\"\"Reads the dicom series and returns the rendered pixel-array, header and dicom file paths.\n",
    "\n",
    "    Notes:\n",
    "        - returned array is in haunsfield uints, you have to take care of windowing\n",
    "    \"\"\"\n",
    "\n",
    "    if series_instance_uid is None:\n",
    "        series_id = sitk.ImageSeriesReader.GetGDCMSeriesIDs(str(study_path))[0]\n",
    "    else:\n",
    "        series_id = series_instance_uid\n",
    "\n",
    "    series_file_names = sitk.ImageSeriesReader.GetGDCMSeriesFileNames(str(study_path), series_id)\n",
    "\n",
    "    headers = list()\n",
    "    for fn in series_file_names:\n",
    "        headers.append(pydicom.dcmread(str(fn), stop_before_pixels=True))\n",
    "\n",
    "    volume = sitk.ReadImage(\n",
    "        series_file_names, sitk.sitkInt32\n",
    "    )\n",
    "#     volume = np.array(sitk.GetArrayFromImage(volume), dtype=np.float32)\n",
    "\n",
    "    if all([i.get('InstanceNumber') is not None for i in headers]):\n",
    "        slice_number_tag = 'InstanceNumber'\n",
    "    elif all([i.get('InstanceNumber') is not None for i in headers]):  # in earlier versions of Dicom\n",
    "        slice_number_tag = 'ImageNumber'\n",
    "    else:\n",
    "        slice_number_tag = None\n",
    "\n",
    "    if slice_number_tag is not None:\n",
    "        sorted_headers = sorted(headers, key=lambda x: int(x.get(slice_number_tag)))\n",
    "        file_name_to_index_mapper = {k: v for v, k in enumerate(series_file_names)}\n",
    "        sorted_file_names = sorted(\n",
    "            series_file_names,\n",
    "            key=lambda x: int(headers[file_name_to_index_mapper[x]].get(slice_number_tag))\n",
    "        )\n",
    "    else:\n",
    "        sorted_headers = headers\n",
    "        sorted_file_names = series_file_names\n",
    "\n",
    "    ret = {\n",
    "        'array': volume,\n",
    "        'headers': sorted_headers,\n",
    "        'dcm_paths': sorted_file_names\n",
    "    }\n",
    "    return ret\n",
    "\n",
    "\n",
    "def apply_windowing(series: np.ndarray,\n",
    "                    window_center: int,\n",
    "                    window_width: int) -> np.ndarray:\n",
    "    \"\"\"Returns an array for given window.\n",
    "\n",
    "    Args:\n",
    "        series: numpy array of shape (n_slices, h, w) or (h, w) in haunsfield units.\n",
    "        window_center: for example, brain window's center is 40\n",
    "        window_width: for example, brain window's width is 80\n",
    "\n",
    "    Returns:\n",
    "        numpy array of shape (n_sclies, h, w) or (h, w) in range(0, 1)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    w_min = int(window_center - (window_width / 2))\n",
    "    w_max = int(window_center + (window_width / 2))\n",
    "\n",
    "    clipped = np.clip(series, w_min, w_max)\n",
    "    windowed = (clipped - w_min) / (w_max - w_min)\n",
    "\n",
    "    return windowed\n",
    "\n",
    "\n",
    "def apply_windowing_using_header(arr: np.ndarray, header: pydicom.FileDataset) -> np.ndarray:\n",
    "    \"\"\"This function returns an array for windows found in windowing dicom tags.\n",
    "\n",
    "    Args:\n",
    "        arr: numpy array of shape (h, w) in haunsfield units.\n",
    "        header: dicom header containing ``WindowCenter`` and ``WindowWidth``\n",
    "\n",
    "    Returns:\n",
    "        numpy array of shape (h, w) in range(0, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    window_center = header.get('WindowCenter')\n",
    "    window_width = header.get('WindowWidth')\n",
    "\n",
    "    return apply_windowing(arr, window_center, window_width)\n",
    "\n",
    "\n",
    "def apply_windowing_using_header_on_series(series: np.ndarray, headers: list[pydicom.FileDataset]) -> np.ndarray:\n",
    "    \"\"\"This function returns an array for windows found in windowing dicom tags.\n",
    "\n",
    "    Args:\n",
    "        series: numpy array of shape (num_slices, h, w) in haunsfield units.\n",
    "        headers: dicom header containing ``WindowCenter`` and ``WindowWidth``\n",
    "\n",
    "    Returns:\n",
    "        numpy array of shape (h, w) in range(0, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    windowed_series = list()\n",
    "    for i, header in enumerate(headers):\n",
    "        window_center = header.get('WindowCenter')\n",
    "        window_width = header.get('WindowWidth')\n",
    "        windowed_series.append(apply_windowing(series[i], window_center, window_width))\n",
    "\n",
    "    return np.array(windowed_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5d78c-4ff0-455d-a329-63032ead9a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prepare_data(df: pd.DataFrame, split: str):\n",
    "    prepared_data_dir_for_split = PREPARED_DATA_DIR / split\n",
    "    prepared_data_dir_for_split.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    prepared_data_dir_for_wcw = PREPARED_DATA_DIR / 'wcw'\n",
    "    prepared_data_dir_for_wcw.mkdir(parents=True, exist_ok=True)\n",
    "    rows_list = list()\n",
    "    for ind, row in tqdm(df.iterrows()):\n",
    "        siuid = row['SeriesInstanceUID']\n",
    "        study_path = DATA_DIR / siuid\n",
    "        series = read_dicom_series(study_path)\n",
    "\n",
    "        prepared_path = prepared_data_dir_for_split / f'{siuid}.nii'\n",
    "        sitk.WriteImage(series['array'], prepared_path)\n",
    "        \n",
    "        window_centers_widths = []\n",
    "        for i, header in enumerate(series['headers']):\n",
    "            window_center = header.get('WindowCenter')\n",
    "            window_width = header.get('WindowWidth')\n",
    "            window_centers_widths.append([window_center, window_width])\n",
    "            \n",
    "        prepared_path_wcw = prepared_data_dir_for_wcw / f'{siuid}.nii'\n",
    "        with open(prepared_path_wcw, 'wb') as f:\n",
    "            np.save(f, np.array(window_centers_widths))\n",
    "        \n",
    "        drow = list()\n",
    "        for elem in list(series['headers'][0]):\n",
    "            drow.append((elem.keyword, elem.value))\n",
    "        rows_list.append(dict(drow))\n",
    "    df = pd.concat([df, pd.DataFrame(rows_list)], axis=1)\n",
    "    df.to_csv(PREPARED_DATA_DIR/\"train.csv\",index=False)\n",
    "    \n",
    "annotations = pd.read_csv(LABELS_PATH)\n",
    "prepare_data(annotations, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87c1342-878a-4ac7-a93f-be66623b401d",
   "metadata": {},
   "source": [
    "## segnpy \n",
    "output dir : seg_npy_data. read segmentation labels, fix sizes and save as npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c68fd33-7295-49c7-b004-b19830d68858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import SimpleITK as sitk\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "\n",
    "seg_dir = Path('/kaggle/input/iaaa-seg/seg')\n",
    "npy_dir = Path('seg')\n",
    "npy_dir.mkdir(exist_ok=True)\n",
    "\n",
    "spaths = [i for i in seg_dir.iterdir()]\n",
    "for i in range(len(spaths)):\n",
    "    if i%50==0:\n",
    "        print(i)\n",
    "        \n",
    "    npy_seg = np.array(sitk.GetArrayFromImage(sitk.ReadImage(spaths[i])), dtype=np.float32)\n",
    "    \n",
    "    seg = torch.from_numpy(npy_seg)\n",
    "    if seg.shape[1]==288:\n",
    "        seg = v2.CenterCrop(size=256)(seg)\n",
    "    else:\n",
    "        seg = v2.Resize(size=256, antialias=True)(seg)\n",
    "    prepared_path = npy_dir / f'{spaths[i].name[:-4]}.npy'\n",
    "    with open(prepared_path, 'wb') as f:\n",
    "        np.save(f, np.float32(seg.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4257d7-e6c0-4bb9-8bd9-620f1daabf70",
   "metadata": {},
   "source": [
    "## iaaa-data-prep \n",
    "output dir: npy_256_fixed. create npy_data.npy file with shape (1035,16,4,256,256) with 4th element of dim 2 as segmentation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e5e10f-bbac-4d08-aa0b-849075e60661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT_DATA_DIR = Path('/kaggle/input/iaaa-nii').expanduser().absolute()\n",
    "DATA_DIR = ROOT_DATA_DIR / 'data'\n",
    "WCW_DIR = '/kaggle/input/iaaa-nii/wcw/'\n",
    "SEG_DIR = '/kaggle/input/seg-npy-data/seg/'\n",
    "LABELS_PATH = ROOT_DATA_DIR / 'train.csv'\n",
    "PREPARED_DATA_DIR = Path('/kaggle/working/').expanduser().absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deea17fe-bd91-4503-965b-5caaf5bad071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "\n",
    "annotations = pd.read_csv(LABELS_PATH)\n",
    "\n",
    "df = annotations.pivot(index='PatientID', columns='SeriesDescription', values=['prediction', 'SeriesInstanceUID','Rows'])\n",
    "df.insert(3, 'label', df.prediction.any(axis=1).astype(np.int32))\n",
    "df = df.reset_index()\n",
    "df.columns = ['PatientID','T1_pred', 'FL_pred', 'T2_pred', 'label', 'T1_siuid','FL_siuid','T2_siuid','T1_rows','FL_rows','T2_rows']\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07df8255-a963-43fa-8bca-ca26a69e0e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sag_list = [146624, 378524, 591219, 331150, 595574, 681963, 684731, 675668, 679925] #331150\n",
    "df = df[~df.PatientID.isin(sag_list)]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891ea18-0a4b-4cde-a2dd-d9216cbba6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import SimpleITK as sitk\n",
    "data = np.zeros((16, 256, 256), dtype=np.float32)\n",
    "with open('zeros.npy', 'wb') as f:\n",
    "    np.save(f, np.float32(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f804d326-508f-4458-8706-d067a21003ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seg'] = ''\n",
    "seg_fnames = [f.name for f in Path(SEG_DIR).iterdir() if f.is_file()]\n",
    "\n",
    "seg_labels = []\n",
    "for i in range(len(df)):\n",
    "    if (df.iloc[i,6] + \".npy\") in seg_fnames:\n",
    "        df.iloc[i,11] = SEG_DIR + df.iloc[i,6] + \".npy\"\n",
    "        seg_npy = np.load(df.iloc[i,11])\n",
    "        labels = np.unique(seg_npy.reshape(-1)).astype(np.int32)\n",
    "        tmp = np.zeros(7)\n",
    "        tmp[labels]=1\n",
    "        seg_labels.append(tmp)\n",
    "    elif (df.iloc[i,7] + \".npy\") in seg_fnames:\n",
    "        df.iloc[i,11] =  SEG_DIR + df.iloc[i,7] + \".npy\"\n",
    "        seg_npy = np.load(df.iloc[i,11])\n",
    "        labels = np.unique(seg_npy.reshape(-1)).astype(np.int32)\n",
    "        tmp = np.zeros(7)\n",
    "        tmp[labels]=1\n",
    "        seg_labels.append(tmp)\n",
    "    elif (df.iloc[i,5] + \".npy\") in seg_fnames:\n",
    "        df.iloc[i,11] =  SEG_DIR + df.iloc[i,7] + \".npy\"\n",
    "        seg_npy = np.load(df.iloc[i,11])\n",
    "        labels = np.unique(seg_npy.reshape(-1)).astype(np.int32)\n",
    "        tmp = np.zeros(7)\n",
    "        tmp[labels]=1\n",
    "        seg_labels.append(tmp)\n",
    "    else:\n",
    "        df.iloc[i,11] = '/kaggle/working/zeros.npy'\n",
    "        tmp = np.zeros(7)\n",
    "        seg_labels.append(tmp)\n",
    "\n",
    "df_seg_label = pd.DataFrame(np.array(seg_labels))\n",
    "df_seg_label.columns = [f'L{i}' for i in range(7)]\n",
    "\n",
    "df = pd.concat([df,df_seg_label], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c363c5b-4588-44ad-ae57-75f79e05b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_npy_file(item):\n",
    "    data = np.load(item)\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "def apply_windowing(series: np.ndarray,\n",
    "                    window_center: int,\n",
    "                    window_width: int) -> np.ndarray:\n",
    "    w_min = int(window_center - (window_width / 2))\n",
    "    w_max = int(window_center + (window_width / 2))\n",
    "\n",
    "    clipped = np.clip(series, w_min, w_max)\n",
    "    windowed = (clipped - w_min) / (w_max - w_min)\n",
    "\n",
    "    return windowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa9197-9a98-4fcb-9b0e-a5a1763d95d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 256,288,256 -> crop center 288 to 256\n",
    "\n",
    "# 240,288,288 -> resize 240 to 288 -> crop center all to 256\n",
    "# 256,288,288 -> resize 256 to 288 -> crop center all to 256\n",
    "# 320,288,288 -> resize 320 to 288 -> crop center all to 256\n",
    "# 288,288,288 -> crop to 256\n",
    "\n",
    "# 256,256,384 -> just resize 384 to 256\n",
    "# 256,256,288 -> just resize 288 to 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d791bc-20fd-454a-bfb3-00a1660337bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data_dir_for_split = PREPARED_DATA_DIR / 'data'\n",
    "prepared_data_dir_for_split.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "prepared_seg_dir_for_split = PREPARED_DATA_DIR / 'seg'\n",
    "prepared_seg_dir_for_split.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "nslices=16\n",
    "print(df.shape)\n",
    "npy_data = np.zeros((1035,16,4,256,256),dtype=np.float32)\n",
    "for i in tqdm(range(len(df))):\n",
    "    if i%50==0:\n",
    "        print(i)\n",
    "\n",
    "    t1 = sitk.ReadImage((DATA_DIR / f'{df.iloc[i,5]}.nii'), sitk.sitkInt32)\n",
    "    fl = sitk.ReadImage((DATA_DIR / f'{df.iloc[i,6]}.nii'), sitk.sitkInt32)\n",
    "    t2 = sitk.ReadImage((DATA_DIR / f'{df.iloc[i,7]}.nii'), sitk.sitkInt32)\n",
    "\n",
    "    npy_t1 = sitk.GetArrayFromImage(t1)\n",
    "    npy_fl = sitk.GetArrayFromImage(fl)\n",
    "    npy_t2 = sitk.GetArrayFromImage(t2)\n",
    "    \n",
    "    npy_seg = np.load(df.iloc[i,11])\n",
    "    #============================================================================\n",
    "    t1w_path = WCW_DIR + df.iloc[i,5] + '.nii'\n",
    "    flw_path = WCW_DIR + df.iloc[i,6] + '.nii'\n",
    "    t2w_path = WCW_DIR + df.iloc[i,7] + '.nii'\n",
    "\n",
    "    npy_t1w = read_npy_file(t1w_path)\n",
    "    npy_flw = read_npy_file(flw_path)\n",
    "    npy_t2w = read_npy_file(t2w_path)\n",
    "    #==========================================================\n",
    "    wnpy_t1 = []\n",
    "    for j in range(len(npy_t1w)):\n",
    "        wnpy_t1.append(apply_windowing(npy_t1[j],npy_t1w[j,0], npy_t1w[j,1]))\n",
    "    npy_t1 = np.array(wnpy_t1)\n",
    "\n",
    "    wnpy_fl = []\n",
    "    for j in range(len(npy_flw)):\n",
    "        wnpy_fl.append(apply_windowing(npy_fl[j],npy_flw[j,0], npy_flw[j,1]))\n",
    "    npy_fl = np.array(wnpy_fl)\n",
    "\n",
    "    wnpy_t2 = []\n",
    "    for j in range(len(npy_t2w)):\n",
    "        wnpy_t2.append(apply_windowing(npy_t2[j],npy_t2w[j,0], npy_t2w[j,1]))\n",
    "    npy_t2 = np.array(wnpy_t2)\n",
    "    #============================================================================\n",
    "    \n",
    "    d_t1 = npy_t1.shape[1]\n",
    "    d_fl = npy_fl.shape[1]\n",
    "    d_t2 = npy_t2.shape[1]\n",
    "    \n",
    "    t_t1 = torch.from_numpy(npy_t1)\n",
    "    t_fl = torch.from_numpy(npy_fl)\n",
    "    t_t2 = torch.from_numpy(npy_t2)\n",
    "    \n",
    "    t_seg = torch.from_numpy(npy_seg)\n",
    "    \n",
    "    \n",
    "    if d_t1==256 and d_fl==288 and d_t2==256:\n",
    "        t_fl = v2.CenterCrop(size=256)(t_fl)\n",
    "        \n",
    "    elif d_fl==288 and d_t2==288:\n",
    "        transform = v2.Compose([v2.Resize(size=288, antialias=True), v2.CenterCrop(size=256)])\n",
    "        t_t1 = transform(t_t1)\n",
    "        t_fl = v2.CenterCrop(size=256)(t_fl)\n",
    "        t_t2 = v2.CenterCrop(size=256)(t_t2)\n",
    "        \n",
    "    elif  d_t1==256 and d_fl==256:\n",
    "        t_t2 = v2.Resize(size=256, antialias=True)(t_t2)\n",
    "        \n",
    "    else:\n",
    "        t_t1 = v2.Resize(size=256, antialias=True)(t_t1)\n",
    "        t_fl = v2.Resize(size=256, antialias=True)(t_fl)\n",
    "        t_t2 = v2.Resize(size=256, antialias=True)(t_t2)\n",
    "\n",
    "    assert ((t_t1.shape[1]==256) and (t_fl.shape[1]==256) and (t_t2.shape[1]==256)), \"shapes must be 256\"\n",
    "#==========================================================================================================        \n",
    "    if t_t1.shape[0]!=8:\n",
    "        sd = t_t1.shape[0] - nslices\n",
    "        selected_indices = torch.clamp(torch.arange(sd//2, nslices+sd//2), 0, len(t_t1)-1)\n",
    "        t_t1 = t_t1[selected_indices]\n",
    "    else:\n",
    "        selected_indices = torch.arange(8).repeat_interleave(2)\n",
    "        t_t1 = t_t1[selected_indices]\n",
    "#==========================================================================================================        \n",
    "    if t_fl.shape[0]!=8:\n",
    "        sd = t_fl.shape[0] - nslices\n",
    "        selected_indices = torch.clamp(torch.arange(sd//2, nslices+sd//2), 0, len(t_fl)-1)\n",
    "        t_fl = t_fl[selected_indices]\n",
    "    else:\n",
    "        selected_indices = torch.arange(8).repeat_interleave(2)\n",
    "        t_fl = t_fl[selected_indices]\n",
    "#==========================================================================================================\n",
    "    if t_t2.shape[0]!=8:\n",
    "        sd = t_t2.shape[0] - nslices\n",
    "        selected_indices = torch.clamp(torch.arange(sd//2, nslices+sd//2), 0, len(t_t2)-1)\n",
    "        t_t2 = t_t2[selected_indices]\n",
    "    else:\n",
    "        selected_indices = torch.arange(8).repeat_interleave(2)\n",
    "        t_t2 = t_t2[selected_indices]\n",
    "#==========================================================================================================\n",
    "    if t_seg.shape[0]!=8:\n",
    "        sd = t_seg.shape[0] - nslices\n",
    "        selected_indices = torch.clamp(torch.arange(sd//2, nslices+sd//2), 0, len(t_seg)-1)\n",
    "        t_seg = t_seg[selected_indices]\n",
    "    else:\n",
    "        selected_indices = torch.arange(8).repeat_interleave(2)\n",
    "        t_seg = t_seg[selected_indices]\n",
    "#==========================================================================================================\n",
    "    npy_data[i,:,0,:,:] = np.float32(t_t1.numpy())\n",
    "    npy_data[i,:,1,:,:] = np.float32(t_fl.numpy())\n",
    "    npy_data[i,:,2,:,:] = np.float32(t_t2.numpy())\n",
    "    npy_data[i,:,3,:,:] = np.float32(t_seg.numpy())\n",
    "\n",
    "npy_data_dir = PREPARED_DATA_DIR / 'npy_data.npy'    \n",
    "with open(npy_data_dir, 'wb') as f:\n",
    "    np.save(f, npy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3fdea-d136-40cc-989f-1331211ce3f4",
   "metadata": {},
   "source": [
    "## iaaa-model-3channel (training script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170edfa1-570c-41a6-82c1-5bdb3631ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd8b6b5-d475-4a90-a5ef-5baa881d8ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "# import pydicom\n",
    "# import SimpleITK as sitk\n",
    "from PIL import Image\n",
    "import click\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, average_precision_score\n",
    "from sklearn.metrics import PrecisionRecallDisplay, precision_recall_curve\n",
    "from sklearn.metrics import RocCurveDisplay, roc_curve\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, GroupShuffleSplit, StratifiedGroupKFold\n",
    "from sklearn.metrics import auc, roc_curve, precision_recall_curve\n",
    "from torch.utils.data import  WeightedRandomSampler\n",
    "from iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold, MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "def read_npy_file(item):\n",
    "    data = np.load(item)\n",
    "    return data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2894deb-1871-40a3-b3ae-82d583835d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_data = np.load('/kaggle/input/npy-256-fixed/npy_data.npy', mmap_mode='r')\n",
    "# npy_data = np.load('/kaggle/input/npy-256-fixed/npy_data.npy')\n",
    "\n",
    "print(npy_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e50feda-36a4-41f5-831e-6ef3fd46cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "LABELS_PATH = '/kaggle/input/iaaa-nii/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec4c70-0a03-4a11-87c4-8243ce731d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "annotations = pd.read_csv(LABELS_PATH)\n",
    "bins = [0,.6,1.2]\n",
    "df = annotations.pivot(index='PatientID', columns='SeriesDescription', values=['prediction', 'SeriesInstanceUID', 'PatientBirthDate','PatientWeight', 'PatientSex'])\n",
    "df = df.reset_index()\n",
    "df.insert(4, 'label', df.prediction.any(axis=1).astype(np.int32))\n",
    "df.iloc[:,8] = (2024-df.iloc[:,8]//10000)\n",
    "df = df.iloc[:,[0,1,2,3,4,5,6,7,8,11,14]]\n",
    "df.columns = ['PatientID','T1_pred','FLAIR_pred','T2_pred','label','T1_SIUID','FLAIR_SIUID','T2_SIUID','AGE','WEIGHT', 'SEX']\n",
    "df.loc[df.AGE>=140,'AGE'] -= 100\n",
    "df['AGE'] = df['AGE'].apply(lambda x: round(x/10)/10)\n",
    "df['SEX'] = df['SEX'].astype('category')\n",
    "df['SEX'] = df['SEX'].cat.codes\n",
    "df['strat'] = df.label.astype(str)+pd.cut(df.AGE,bins=bins,labels=False).astype(str)# + df.SEX.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0864470e-636c-4e0d-9eb2-fff20dea9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sag_list = [146624, 378524, 591219, 331150, 595574, 681963, 684731, 675668, 679925] #331150\n",
    "df = df[~df.PatientID.isin(sag_list)]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71e17e-0f05-4483-8db9-d878a4b79e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seg'] = ''\n",
    "# 1035,16,4,256,256\n",
    "seg_labels = []\n",
    "for i in range(len(df)):\n",
    "    seg_npy = npy_data[i,:,3,:,:]\n",
    "    labels = np.unique(seg_npy.reshape(-1)).astype(np.int32)\n",
    "    tmp = np.zeros(7)\n",
    "    tmp[labels]=1\n",
    "    seg_labels.append(tmp)\n",
    "\n",
    "df_seg_label = pd.DataFrame(np.array(seg_labels))\n",
    "df_seg_label.columns = [f'L{i}' for i in range(7)]\n",
    "\n",
    "df = pd.concat([df,df_seg_label], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca0197-864f-4549-a223-77c5b09cc8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, annotations, data, transform, training):\n",
    "        self.labels = annotations\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.training = training\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        ix = self.labels.loc[idx, 'original_index']\n",
    "        age = self.labels.loc[idx, ['AGE','WEIGHT','SEX']].values.astype(np.float32)#         arr = np.copy(self.data[self.labels.index[idx]])\n",
    "        lbl = self.labels.loc[idx, 'label']\n",
    "        label = torch.from_numpy(self.labels.loc[idx,['T1_pred','FLAIR_pred','T2_pred']].values.astype(np.float32))\n",
    "        arr = np.copy(self.data[ix])\n",
    "\n",
    "        images = torch.from_numpy(arr).to(torch.float32)\n",
    "\n",
    "#==========================================================================================================\n",
    "        if self.training:\n",
    "            images = torch.stack([self.transform(img) for img in images])\n",
    "        else:\n",
    "            images = self.transform(images)\n",
    "        if self.training:\n",
    "            images = images[torch.randperm(len(images))]\n",
    "        \n",
    "        segmentation_labels = torch.round(images[:,3,:,:])\n",
    "        one_hot_encoded_labels = torch.zeros((16, 7), dtype=torch.float32)\n",
    "        for i in range(segmentation_labels.shape[0]):\n",
    "            unique_values = segmentation_labels[i].unique()  # Get unique values in the mask\n",
    "            one_hot_encoded_labels[i, unique_values.to(torch.int32)] = 1  # Set the corresponding positions to 1\n",
    "        \n",
    "        images = images[:,:3]        \n",
    "        return images, torch.tensor(age,dtype=torch.float32), label, one_hot_encoded_labels[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59cc87-7ff8-4ba1-b4bd-ad2a586ee2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = v2.Compose([\n",
    "    v2.RandomAffine(degrees=180,\n",
    "                    translate=(0.1, 0.1),\n",
    "                    scale=(0.95, 1.05),\n",
    "                    shear=(-10,10,-10,10)\n",
    "                   ),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32),\n",
    "])\n",
    "transformsv = v2.Compose([\n",
    "    v2.ToDtype(torch.float32),    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d74d44-71ba-4b25-afdd-f75b6f1dedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class effb0_meta(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(effb0_meta, self).__init__()\n",
    "        self.model = models.efficientnet_b0(weights='DEFAULT')\n",
    "        self.model2 = models.efficientnet_b0(weights='DEFAULT')\n",
    "        self.model.classifier[1] = nn.Identity()\n",
    "        self.model2.classifier[1] = nn.Identity()\n",
    "        self.fc1 = nn.Linear(1280,6)\n",
    "        self.fc2 = nn.Linear(1280,6)\n",
    "        self.fc3 = nn.Linear(1280,6)\n",
    "        self.fc4 = nn.Linear(1280,3)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        bs,sl,ch,h,w = x.size()\n",
    "        x = x.contiguous()\n",
    "        x1 = x.clone()\n",
    "        \n",
    "        x = x.view(bs*sl,ch,h,w)\n",
    "        x = self.model(x)\n",
    "        x = x.view(bs,sl,1280)\n",
    "        o1 = torch.stack((self.fc1(x), self.fc2(x), self.fc3(x)), dim=2)\n",
    "        x1 = x1.permute(0,2,1,3,4).view(bs, 3, 4, 4, 256, 256).permute(0, 1, 2, 4, 3, 5).reshape(bs, 3, 4 * 256, 4 * 256)\n",
    "        x1 = self.model2(x1)\n",
    "        \n",
    "        o2 = self.fc4(x1)\n",
    "        \n",
    "        return o1, o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc2787-acc0-493a-94fe-e2c4b3be4fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, filename='checkpoint.pth'):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, filename)\n",
    "\n",
    "# Function to load a checkpoint\n",
    "def load_checkpoint(model, optimizer, filename='checkpoint.pth'):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch'], checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18efad00-d8ca-4191-9eb7-2e5994adccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5caeca-da26-48b1-9098-b076a364fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_dir = Path('figs')\n",
    "figs_dir.mkdir(exist_ok=True)\n",
    "num_epochs=30\n",
    "bs=4\n",
    "sss = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=13)\n",
    "for i, (train_index, val_index) in enumerate(sss.split(df.PatientID, df.iloc[:,-6:])):\n",
    "    dftr = df.iloc[train_index]\n",
    "    dftr = dftr.reset_index().rename(columns={'index': 'original_index'})\n",
    "    \n",
    "    dfte = df.iloc[val_index]\n",
    "    dfte = dfte.reset_index().rename(columns={'index': 'original_index'})\n",
    "    \n",
    "    training_data = MRIDataset(dftr, npy_data, transform = transforms, training = True)\n",
    "    validation_data = MRIDataset(dfte, npy_data, transform = transformsv, training = False)\n",
    "    train_loader = DataLoader(training_data, batch_size=bs, shuffle=True, num_workers=4,pin_memory=True)\n",
    "    val_loader = DataLoader(validation_data, batch_size=bs, shuffle=False, num_workers=4,pin_memory=True)\n",
    "    \n",
    "    model = effb0_meta()\n",
    "    model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "    criterion2 = nn.MSELoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.0003,\n",
    "                                                  step_size_up=int(len(train_loader)*0.42),\n",
    "                                                  cycle_momentum=False)\n",
    "    best_ap = 0.0\n",
    "    learning_rates = [0.00001] + [0.0001 for i in range(16)] + [0.00001 for i in range(3)]\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_loss2 = 0.0\n",
    "        train_mseloss = 0.0\n",
    "        train_preds = []\n",
    "        train_preds2 = []\n",
    "        train_preds3 = []\n",
    "        train_targets = []\n",
    "        for inputs, ages, targets, auxt in train_loader:\n",
    "            # Move the inputs and targets to the GPU\n",
    "            inputs = inputs.to(device, non_blocking=True)     #(bs,16,3,256,256)\n",
    "            ages = ages.to(device, non_blocking=True)         # (bs,3)\n",
    "            targets = targets.to(device, non_blocking=True)   #(bs,3)\n",
    "            auxt = auxt.to(device, non_blocking=True)         #(bs,16,256,256)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, outputs2 = model(inputs,ages)\n",
    "            bs,sl,ch,lbl = outputs.size()\n",
    "            newTargets = torch.zeros((bs, sl, ch, lbl), dtype=auxt.dtype)\n",
    "            targets_expanded = targets.unsqueeze(1).expand(-1, sl, -1)\n",
    "            mask = targets_expanded == 1  # Shape (4, 16, 3)\n",
    "            newTargets = auxt.unsqueeze(2) * mask.unsqueeze(-1).to(auxt.dtype) #same shape as outputs (4,16,3,6)\n",
    "            losses = criterion(outputs[:,:,0], newTargets[:,:,0].float())\n",
    "            losses += criterion(outputs[:,:,1], newTargets[:,:,1].float())\n",
    "            losses += criterion(outputs[:,:,2], newTargets[:,:,2].float())\n",
    "\n",
    "            losses2 = criterion(outputs2, targets)\n",
    "            \n",
    "            losses.backward()\n",
    "            losses2.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += losses.item()\n",
    "            train_loss2 += losses2.item()\n",
    "            pro1 = outputs.permute(0,1,3,2)[:,:,torch.tensor([1,2,3,4]),:].reshape(-1,16*4,3).max(-2).values.view(-1)\n",
    "            pro2 = outputs2.view(-1)\n",
    "            pro3 = (pro1+pro2)/2.0\n",
    "            train_preds.extend(torch.sigmoid(pro1).tolist())\n",
    "            train_preds2.extend(torch.sigmoid(pro2).tolist())\n",
    "            train_preds3.extend(torch.sigmoid(pro3).tolist())\n",
    "\n",
    "            train_targets.extend(targets.view(-1).tolist())\n",
    "        train_loss /= len(train_loader)\n",
    "        train_mseloss /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_loss2 = 0.0\n",
    "        val_mseloss = 0.0\n",
    "        val_preds = []\n",
    "        val_preds2 = []\n",
    "        val_preds3 = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, ages, targets, auxt in val_loader:\n",
    "                # Move the inputs and targets to the GPU\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                ages = ages.to(device, non_blocking=True)\n",
    "                targets = targets.to(device, non_blocking=True)\n",
    "                auxt = auxt.to(device, non_blocking=True)\n",
    "\n",
    "                outputs, outputs2 = model(inputs,ages)\n",
    "                bs,sl,ch,lbl = outputs.size()\n",
    "                newTargets = torch.zeros((bs, sl, ch, lbl), dtype=auxt.dtype)\n",
    "                targets_expanded = targets.unsqueeze(1).expand(-1, sl, -1)\n",
    "                mask = targets_expanded == 1  # Shape (bs, 16, 3)\n",
    "                newTargets = auxt.unsqueeze(2) * mask.unsqueeze(-1).to(auxt.dtype) #same shape as outputs (bs,16,3,6)\n",
    "                losses = criterion(outputs[:,:,0], newTargets[:,:,0].float())\n",
    "                losses += criterion(outputs[:,:,1], newTargets[:,:,1].float())\n",
    "                losses += criterion(outputs[:,:,2], newTargets[:,:,2].float())\n",
    "\n",
    "                losses2 = criterion(outputs2, targets)\n",
    "\n",
    "                val_loss += losses.item()\n",
    "                val_loss2 += losses2.item()\n",
    "        \n",
    "                pro1 = outputs.permute(0,1,3,2)[:,:,torch.tensor([1,2,3,4]),:].reshape(-1,16*4,3).max(-2).values.view(-1)\n",
    "                pro2 = outputs2.view(-1)\n",
    "                pro3 = (pro1+pro2)/2.0\n",
    "                val_preds.extend(torch.sigmoid(pro1).tolist())\n",
    "                val_preds2.extend(torch.sigmoid(pro2).tolist())\n",
    "                val_preds3.extend(torch.sigmoid(pro3).tolist())\n",
    "                val_targets.extend(targets.view(-1).tolist())\n",
    "                \n",
    "        scheduler.step()        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_mseloss /= len(val_loader)\n",
    "        val_auc = roc_auc_score(val_targets, val_preds)\n",
    "        val_ap = average_precision_score(val_targets, val_preds)\n",
    "        val_ap2 = average_precision_score(val_targets, val_preds2)\n",
    "        val_ap3 = average_precision_score(val_targets, val_preds3)\n",
    "        val_precision = precision_score(val_targets, [1 if p > 0.5 else 0 for p in val_preds])\n",
    "        val_recall = recall_score(val_targets, [1 if p > 0.5 else 0 for p in val_preds])\n",
    "        \n",
    "        fpr, tpr, thresholds_roc = roc_curve(val_targets, val_preds)\n",
    "        val_roc_auc = auc(fpr, tpr)\n",
    "        precision, recall, thresholds_pr = precision_recall_curve(val_targets, val_preds)\n",
    "        val_pr_auc = auc(recall, precision)\n",
    "        \n",
    "        train_ap = average_precision_score(train_targets, train_preds)\n",
    "        train_ap2 = average_precision_score(train_targets, train_preds2)\n",
    "        train_ap3 = average_precision_score(train_targets, train_preds3)\n",
    "        \n",
    "        end_time = time.time()  # End time for the epoch\n",
    "        epoch_duration = end_time - start_time  # Calculate duration\n",
    "        print(f\"Fold [{i+1}], ({epoch_duration:.2f}s) Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f},    Train Loss2: {train_loss2:.4f},    Train AP: {train_ap:.4f},    Train AP2: {train_ap2:.4f},    Train AP3: {train_ap3:.4f},    Val Loss: {val_loss:.4f},    Val Loss2: {val_loss2:.4f},    Val AUC: {val_auc:.4f},    Val AP: {val_ap:.4f},    Val AP2: {val_ap2:.4f},    Val AP3: {val_ap3:.4f},    Val Precision: {val_precision:.4f},    Val Recall: {val_recall:.4f},    Val roc_auc: {val_roc_auc:.4f},    Val pr_auc: {val_pr_auc:.4f}\")\n",
    "        \n",
    "        # Create a figure with subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        RocCurveDisplay.from_predictions(val_targets, val_preds, ax=ax1)\n",
    "        ax1.set_title('ROC Curve')\n",
    "        ax1.set_xlabel('False Positive Rate')\n",
    "        ax1.set_ylabel('True Positive Rate')\n",
    "        PrecisionRecallDisplay.from_predictions(val_targets, val_preds, ax=ax2)\n",
    "        ax2.set_title('Precision-Recall Curve')\n",
    "        ax2.set_xlabel('Recall')\n",
    "        ax2.set_ylabel('Precision')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"figs/fold{i+1}_epoch{epoch+1}\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        if val_pr_auc > best_ap:\n",
    "            best_ap = val_pr_auc\n",
    "        save_checkpoint(model, optimizer, epoch, val_loss, filename= f\"chechpoint_fold{i+1}_epoch{epoch+1}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
